{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for Non-Linear Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between $x$ and $y$ in this data is non-linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.linspace(0, 20, 100)\n",
    "y = .5 * np.sin(.5*x) + np.random.normal(0, .2, 100)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, we can still fit it using linear regression. Let's try to fit a function of the form:\n",
    "\n",
    "$$b_0 + b_1 x + b_2 x^2 + b_3 x^3 + b_4 x^4.$$\n",
    "\n",
    "\n",
    "The design matrix is \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    1 & x_1 & x_1^2 & x_1^3 & x_1^4\\\\\n",
    "    1 & x_2 & x_2^2 & x_2^3 & x_2^4\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    1 & x_n & x_n^2 & x_n^3 & x_n^4\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((\n",
    "    np.ones_like(x),\n",
    "    x,\n",
    "    x**2,\n",
    "    x**3,\n",
    "    x**4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we solve the normal equations, $X^\\intercal X \\vec b = X^\\intercal \\vec y$. We can do this using `np.linalg.solve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the predicted $y$ coordinate of every point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = b[0] + b[1]*x + b[2]*x**2 + b[3]*x**3 + b[4]*x**4\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember, the prediction vector $\\vec h$ is also given by $X \\vec b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, X @ b, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember what $\\vec b$ was?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that numpy has a function to find the least squares regression parameters for fitting a polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(x, y, deg=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these are the same parameters we recovered, just in reverse order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting higher order polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are trying to fit a more complicated pattern, we might try using a higher order polynomial (it will have more \"wiggles\"). But the system of equations obtained when trying to fit a high-order polynomial is often \"ill conditioned\", meaning that the small numerical errors resulting from the finite precision of computer math may lead to big inaccuracies in the final result.\n",
    "\n",
    "For example, consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = x % 2\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, any $n$ points in two dimensions can be fit *exactly* by a polynomial of degree $n+1$. For instance, any two points is fit exactly by some line, any three points can be fit by some quadratic, any four points can be fit by some cubic, and so on. So in principle, there is a polynomial of degree $21$ which fits the above data exactly. But look what happens when we try to fit a polynomial with least squares regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.polyfit(x, x%2, deg=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll get an answer, but you probably got an ugly red message saying \"Polyfit may be poorly conditioned\". Moreover, the plot of the predictions is not so good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0, 20, 100)\n",
    "predictions = np.column_stack(xx**p for p in range(22)) @ b[::-1]\n",
    "\n",
    "plt.plot(xx, predictions)\n",
    "plt.scatter(x, x%2)\n",
    "plt.ylim([-2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while high-order polynomials can, in theory, describe complicated patterns, in practice they are generally unsuitable for regression due to numerical issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
